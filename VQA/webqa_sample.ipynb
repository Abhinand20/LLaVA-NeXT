{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_dir = '/mnt/disks/data/webqa/WebQA_train_val.json'\n",
    "with open(dataset_dir, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = {}\n",
    "for id,dict in data.items():\n",
    "    if dict['Qcate'] == 'text':\n",
    "        continue\n",
    "    elif not dict['split'] == 'val':\n",
    "        continue\n",
    "    elif not len(dict['img_posFacts']) > 0:\n",
    "        continue\n",
    "    val_data[id] = dict\n",
    "\n",
    "print(len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "lineidx_dir = '/mnt/disks/data/webqa/imgs.lineidx'\n",
    "images_dir = '/mnt/disks/data/webqa/imgs.tsv'\n",
    "\n",
    "with open(lineidx_dir, \"r\") as fp_lineidx:\n",
    "    lineidx = [int(i.strip()) for i in fp_lineidx.readlines()]\n",
    "\n",
    "def read_image(image_id):\n",
    "    with open(images_dir, \"r\") as fp:\n",
    "        fp.seek(lineidx[int(image_id)%10000000])\n",
    "        imgid, img_base64 = fp.readline().strip().split('\\t')\n",
    "    assert int(image_id) == int(imgid), f'{image_id} {imgid}'\n",
    "    im = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multihop_prompt(ques, titles, captions):\n",
    "    question = f\"{DEFAULT_IMAGE_TOKEN} This is the first image.\\nTitle: {titles[0]}\\nCaption: {captions[0]}\\n\\nNow, let's look at another image: {DEFAULT_IMAGE_TOKEN} Title: {titles[1]}\\nCaption: {captions[1]}\\nRespond with the correct answer to the following question in a complete sentence. Question: {ques}\"\n",
    "    return question\n",
    "\n",
    "def get_si_prompt(ques, titles, captions):\n",
    "    question = f\"{DEFAULT_IMAGE_TOKEN} You have to answer a question about this image\\nTitle: {titles[0]}\\nCaption: {captions[0]}\\nRespond with the correct answer to the following question in a complete sentence. Question: {ques}\"\n",
    "    return question\n",
    "\n",
    "def get_answer(data, model, image_processor, tokenizer, max_len, device):\n",
    "    ques = data['Q']\n",
    "    images = []\n",
    "    titles = []\n",
    "    captions = []\n",
    "    for img in data['img_posFacts']:\n",
    "        images.append(read_image(img['image_id']))\n",
    "        titles.append(img['title'])\n",
    "        captions.append(img['caption'])\n",
    "    if len(images) == 2:\n",
    "        prompt = get_multihop_prompt(ques, titles, captions)\n",
    "    else:\n",
    "        prompt = get_si_prompt(ques, titles, captions)\n",
    "    image_tensors = process_images(images, image_processor, model.config)\n",
    "    image_tensors = [_image.to(dtype=torch.float16, device=device) for _image in image_tensors]\n",
    "    \n",
    "    conv_template = \"qwen_1_5\"\n",
    "    conv = copy.deepcopy(conv_templates[conv_template])\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt_question = conv.get_prompt()\n",
    "\n",
    "    input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "    image_sizes = [image.size for image in images]\n",
    "    print(prompt_question)\n",
    "    # Generate response\n",
    "    cont = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensors,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0,\n",
    "        max_new_tokens=4096,\n",
    "    )\n",
    "    text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "    return text_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for id, d in val_data.items():\n",
    "    if len(d['img_posFacts']) > 1:\n",
    "        count += 1\n",
    "        print(d)\n",
    "        if count > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "# pretrained = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "pretrained = \"lmms-lab/llava-onevision-qwen2-7b-ov\"\n",
    "model_name = \"llava_qwen\"\n",
    "device = \"cuda\"\n",
    "device_map = \"auto\"\n",
    "llava_model_args = {\n",
    "        \"multimodal\": True,\n",
    "    }\n",
    "overwrite_config = {}\n",
    "overwrite_config[\"image_aspect_ratio\"] = \"pad\"\n",
    "llava_model_args[\"overwrite_config\"] = overwrite_config\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map, load_8bit=True, **llava_model_args)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for id,d in val_data.items():\n",
    "    count += 1\n",
    "    ans = get_answer(d, model, image_processor, tokenizer, max_length, device)\n",
    "    print(f\"Ques: {d['Q']}\\nAssistant: {ans}\\nGT: {d['A'][0]}\\n\")\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map test set images to IDs\n",
    "import json\n",
    "test_dir = '/mnt/disks/data/webqa/WebQA_test.json'\n",
    "retrieved_dir = '/mnt/disks/data/webqa/WebQA_retrieved_test.json'\n",
    "\n",
    "with open(test_dir, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "with open(retrieved_dir, 'r') as f:\n",
    "    retrieved_data = json.load(f)\n",
    "\n",
    "img_url_to_id = {}\n",
    "\n",
    "for qid,d in test_data.items():\n",
    "    for img in d['img_Facts']:\n",
    "        img_url_to_id[img['imgUrl']] = img['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(img_url_to_id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "t = defaultdict(int)\n",
    "i = defaultdict(int)\n",
    "for id,d in retrieved_data.items():\n",
    "    t[len(d['txt_Facts'])] += 1\n",
    "    i[len(d['img_Facts'])] += 1\n",
    "\n",
    "print(t)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image context (1,2) + Text context (1,2) + Question\n",
    "# For cases where Img context = 0; use multimodal = False config and re-run the generation\n",
    "def get_multihop_image_prompt(titles):\n",
    "    question = f\"{DEFAULT_IMAGE_TOKEN} This is the first image.\\nTitle: {titles[0]}\\n\\nNow, let's look at another image: {DEFAULT_IMAGE_TOKEN} Title: {titles[1]}\\n\"\n",
    "    return question\n",
    "\n",
    "def get_si_prompt(titles):\n",
    "    question = f\"{DEFAULT_IMAGE_TOKEN} Consider this image\\nTitle: {titles[0]}\\n\"\n",
    "    return question\n",
    "\n",
    "def get_multihop_text_propmpt(texts):\n",
    "    question = f\"Fact 1: {texts[0]}\\n\\nFact 2: {texts[1]}\\n\"\n",
    "    return question\n",
    "\n",
    "def get_single_text_propmpt(texts):\n",
    "    question = f\"Fact: {texts[0]}\\n\"\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1340"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate score on the val set\n",
    "import json\n",
    "\n",
    "answers = \"/mnt/disks/data/webqa/ans_val_imgs_base.txt\"\n",
    "with open(\"/mnt/disks/data/webqa/WebQA_train_val.json\", 'r') as f:\n",
    "    train_val_dataset = json.load(f)\n",
    "\n",
    "ans_data = {}\n",
    "with open(answers, 'r') as f:\n",
    "    for row in f:\n",
    "        qid, res = row.split(\"\\t\")\n",
    "        ans_data[qid] = res.strip()\n",
    "eval_data = {k:v for k, v in train_val_dataset.items() if v['split'] == 'val' and v['Qcate'].lower() in ['color', 'shape', 'yesno', 'number']}\n",
    "len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_1022 import *\n",
    "import sys\n",
    "qa_results = {}\n",
    "for k in eval_data.keys():\n",
    "    question = eval_data[k]['Q']\n",
    "    if k not in ans_data:\n",
    "        continue\n",
    "    answer = [ans_data[k]]\n",
    "    label = eval_data[k]['A']\n",
    "\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    if Qcate == 'color': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(answer, label[0], \"\", color_set)\n",
    "        sys.stdout.flush()\n",
    "    elif Qcate == 'shape': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(answer, label[0], \"\", shape_set)\n",
    "    elif Qcate == 'yesno': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(answer, label[0], \"\", yesno_set)\n",
    "    elif Qcate == 'number': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(answer, label[0], \"\", {\"NUMBER\"})\n",
    "    else:\n",
    "        continue\n",
    "    if not Qcate in qa_results:\n",
    "        qa_results[Qcate] = []\n",
    "    qa_results[Qcate].append(PR_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yesno: 0.6034271725826194\n",
      "shape: 0.3926940639269406\n",
      "color: 0.7430167597765364\n",
      "number: 0.556758530183727\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for k,v in qa_results.items():\n",
    "    print(f\"{k}: {np.mean(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llavanext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
