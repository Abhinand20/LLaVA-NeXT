{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do an eval for answers we get from LLaVa-Next to understand the following\n",
    "\n",
    "- Baseline: Single vs 2-image accuracy\n",
    "- Perturbed images + labels unchanged: Single vs 2-image accuracy\n",
    "- Perturbed images + new labels: Single vs 2-image accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import os.path as osp\n",
    "\n",
    "disk_root = \"/mnt/disks/data/webqa\"\n",
    "# LLaVa next predictions\n",
    "llavan_train_color_path = osp.join(disk_root, \"llavanext_webqa_ptb_train_color.jsonl\")\n",
    "llavan_val_color_path = osp.join(disk_root, \"llavanext_webqa_ptb_val_color.jsonl\")\n",
    "# Candidate questions\n",
    "candidate_path = osp.join(disk_root, \"ptb_color_gpt_validated.json\")\n",
    "\n",
    "with open(candidate_path, 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "predictions = {}\n",
    "for in_file in [llavan_train_color_path, llavan_val_color_path]:\n",
    "    with jsonlines.open(in_file, 'r') as f:\n",
    "        for obj in f:\n",
    "            key = list(obj.keys())[0]\n",
    "            val = list(obj.values())[0]\n",
    "            predictions[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Do we have predictions for all questions?\n",
    "remaining = set()\n",
    "for k in eval_data.keys():\n",
    "    if k not in predictions:\n",
    "        remaining.add(k)\n",
    "\n",
    "print(len(remaining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_1022 import compute_vqa_metrics, color_set, shape_set, yesno_set\n",
    "import numpy as np\n",
    "\n",
    "def webqa_accuracy(answer, label, Qcate):\n",
    "    if Qcate == 'color':\n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", color_set)\n",
    "    elif Qcate == 'shape': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", shape_set)\n",
    "    elif Qcate == 'yesno': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", yesno_set)\n",
    "    elif Qcate == 'number': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", {\"NUMBER\"})\n",
    "    else:\n",
    "        return None\n",
    "    return (F1_avg, F1_max, EM, RE_avg, PR_avg)\n",
    "\n",
    "def accuracy_agg_results(qa_results):\n",
    "    single_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 1]\n",
    "    two_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 2]\n",
    "\n",
    "    single_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items() if key in single_image_keys])\n",
    "    two_image_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items() if key in two_image_keys])\n",
    "    avr_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items()])\n",
    "    return (single_acc, two_image_acc, avr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLAVA-NEXT baseline on original images that have perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8095760233918129, 0.8744252873563219, 0.8227272727272728)\n"
     ]
    }
   ],
   "source": [
    "llavan_results_baseline = {}\n",
    "for k in list(eval_data.keys()):\n",
    "    question = eval_data[k]['Q']\n",
    "    answer = predictions[k]['llava_A']\n",
    "    label = eval_data[k]['A']\n",
    "    eval_data[k]['A_llavanext'] = answer\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    llavan_results_baseline[k] = webqa_accuracy(answer, label, Qcate)\n",
    "\n",
    "print(accuracy_agg_results(llavan_results_baseline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct despite perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llavan_results_perturbed_original_label = {}\n",
    "llavan_results_perturbed_generated_label = {}\n",
    "\n",
    "for k in list(eval_data.keys()):\n",
    "    llavan_results_perturbed_original_label[k] = {}\n",
    "    llavan_results_perturbed_generated_label[k] = {}\n",
    "    eval_data[k]['A_perturbed_llavanext'] = {}\n",
    "    question = eval_data[k]['Q']\n",
    "    for idx, label in eval_data[k]['A_perturbed'].items():\n",
    "        answer = predictions[k]['llava_A_perturbed'][int(idx)]\n",
    "        original_label = eval_data[k]['A']\n",
    "        eval_data[k]['A_perturbed_llavanext'][idx] = answer\n",
    "        Qcate = eval_data[k]['Qcate'].lower()\n",
    "        llavan_results_perturbed_original_label[k][idx] = webqa_accuracy(answer, original_label, Qcate)\n",
    "        llavan_results_perturbed_generated_label[k][idx] = webqa_accuracy(answer, [label], Qcate)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.13804713804713803, 0.5819277108433735, 0.22382615444315093)\n",
      "(0.7188552188552187, 0.34508032128514055, 0.6466239813736904)\n"
     ]
    }
   ],
   "source": [
    "def accuracy_agg_generated_results(qa_results):\n",
    "    single_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 1]\n",
    "    two_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 2]\n",
    "\n",
    "    single_acc = np.mean([PR_avg for key, dict in qa_results.items() if key in single_image_keys for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    two_image_acc = np.mean([PR_avg for key, dict in qa_results.items() if key in two_image_keys for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    avr_acc = np.mean([PR_avg for key, dict in qa_results.items() for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    \n",
    "    return (single_acc, two_image_acc, avr_acc)\n",
    "\n",
    "print(accuracy_agg_generated_results(llavan_results_perturbed_original_label))\n",
    "print(accuracy_agg_generated_results(llavan_results_perturbed_generated_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Blank predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment name</th>\n",
       "      <th>single_image</th>\n",
       "      <th>two_image</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.809576</td>\n",
       "      <td>0.874425</td>\n",
       "      <td>0.822727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perturbed_original_label</td>\n",
       "      <td>0.138047</td>\n",
       "      <td>0.581928</td>\n",
       "      <td>0.223826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perturbed_generated_label</td>\n",
       "      <td>0.718855</td>\n",
       "      <td>0.345080</td>\n",
       "      <td>0.646624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             experiment name  single_image  two_image   average\n",
       "0                   baseline      0.809576   0.874425  0.822727\n",
       "1   perturbed_original_label      0.138047   0.581928  0.223826\n",
       "2  perturbed_generated_label      0.718855   0.345080  0.646624"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "exp_name = \"llavanext\"\n",
    "baseline_accs = accuracy_agg_results(llavan_results_baseline)\n",
    "perturbed_original_label_accs = accuracy_agg_generated_results(llavan_results_perturbed_original_label)\n",
    "perturbed_generated_label_acc = accuracy_agg_generated_results(llavan_results_perturbed_generated_label)\n",
    "\n",
    "columns = ['experiment name', 'single_image', 'two_image', 'average']\n",
    "accuracy_agg_df = pd.DataFrame(columns=columns)\n",
    "accuracy_agg_df['experiment name'] = ['baseline', 'perturbed_original_label', 'perturbed_generated_label']\n",
    "accuracy_agg_df['single_image'] = [baseline_accs[0], perturbed_original_label_accs[0], perturbed_generated_label_acc[0]]\n",
    "accuracy_agg_df['two_image'] = [baseline_accs[1], perturbed_original_label_accs[1], perturbed_generated_label_acc[1]]\n",
    "accuracy_agg_df['average'] = [baseline_accs[2], perturbed_original_label_accs[2], perturbed_generated_label_acc[2]]\n",
    "accuracy_agg_df.to_csv(\"results/{exp_name}.csv\", index=False)\n",
    "accuracy_agg_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llavanext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
